{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install autofeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from autofeat import AutoFeatRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "np.seterr(divide = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regr adatbeolvasó fv.\n",
    "def load_regression_dataset(name):\n",
    "    units = {}\n",
    "    if name == \"boston\":\n",
    "        X, y = load_boston(True)\n",
    "    elif name == \"diabetes\":\n",
    "        X, y = load_diabetes(True)\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown dataset %r\" % name)\n",
    "    return np.array(X, dtype=float), np.array(y, dtype=float), units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#módosított modell tesztelő fv.\n",
    "def test_model(X, y, model, param_grid):\n",
    "    # load data\n",
    "    #X, y, _ = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    \n",
    "    if model.__class__.__name__ == \"SVR\":\n",
    "        sscaler = StandardScaler()\n",
    "        X_train = sscaler.fit_transform(X_train)\n",
    "        X_test = sscaler.transform(X_test)\n",
    "        \n",
    "    # train model on train split incl cross-validation for parameter selection\n",
    "    gsmodel = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    gsmodel.fit(X_train, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test)))\n",
    "    return gsmodel.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#módosított autofeat modell tesztelő fv.\n",
    "def test_autofeat(X, y, units, feateng_steps=2):\n",
    "    # load data\n",
    "    #X, y, units = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    # run autofeat\n",
    "    afreg = AutoFeatRegressor(verbose=1, feateng_steps=feateng_steps, units=units)\n",
    "    # fit autofeat on less data, otherwise ridge reg model with xval will overfit on new features\n",
    "    X_train_tr = afreg.fit_transform(X_train, y_train)\n",
    "    X_test_tr = afreg.transform(X_test)\n",
    "    print(\"autofeat new features:\", len(afreg.new_feat_cols_))\n",
    "    print(\"autofeat MSE on training data:\", mean_squared_error(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat MSE on test data:\", mean_squared_error(y_test, afreg.predict(X_test_tr)))\n",
    "    print(\"autofeat R^2 on training data:\", r2_score(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat R^2 on test data:\", r2_score(y_test, afreg.predict(X_test_tr)))\n",
    "    \n",
    "    \n",
    "    # train rreg on transformed train split incl cross-validation for parameter selection\n",
    "    print(\"# Ridge Regression\")\n",
    "    rreg = Ridge()\n",
    "    param_grid = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000.]}\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gsmodel = GridSearchCV(rreg, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "        gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    \n",
    "    \n",
    "    print(\"# Random Forest\")\n",
    "    rforest = RandomForestRegressor(n_estimators=100, random_state=13)\n",
    "    param_grid = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "    gsmodel = GridSearchCV(rforest, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    \n",
    "    \n",
    "    print(\"# SVR\")\n",
    "    svr = SVR(gamma=\"scale\")\n",
    "    param_grid = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "    sscaler = StandardScaler()\n",
    "    X_train_tr = sscaler.fit_transform(X_train_tr)\n",
    "    X_test_tr = sscaler.transform(X_test_tr)\n",
    "    gsmodel = GridSearchCV(svr, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoFeat] The 2 step feature engineering process could generate up to 4186 features.\n",
      "[AutoFeat] With 506 data points this new feature matrix would use about 0.01 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 60 transformed features from 13 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 2583 feature combinations from 2628 original feature tuples - done.\n",
      "[feateng] Generated altogether 2645 new features in 2 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 1198 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 54 features after 5 feature selection runs\n",
      "[featsel] 43 features after correlation filtering\n",
      "[featsel] 21 features after noise filtering\n",
      "[AutoFeat] Computing 20 new features.\n",
      "[AutoFeat]    20/   20 new features ...done.\n",
      "[AutoFeat] Final dataframe with 33 feature columns (20 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "4.208036056309933\n",
      "42.588235 * 1/(x007*x012)\n",
      "-21.647112 * log(x004)/x007\n",
      "10.819415 * 1/(x002*x007)\n",
      "5.625274 * x005**3/x009\n",
      "3.480543 * sqrt(x011)/x010\n",
      "2.306911 * x005**2/x010\n",
      "0.103561 * x000**2*x003\n",
      "-0.083379 * x000\n",
      "0.001332 * exp(x005)*log(x007)\n",
      "-0.000703 * x006**2/x008\n",
      "-0.000570 * x005**3*x012\n",
      "0.000313 * x005**3*sqrt(x011)\n",
      "-0.000092 * exp(x005)*log(x000)\n",
      "-0.000029 * x009**2/x008\n",
      "[AutoFeat] Final score: 0.9072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(506, 33)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adatok beolvasása\n",
    "X, y, units = load_regression_dataset(\"boston\")\n",
    "#új változók generálása autofeattel\n",
    "afreg = AutoFeatRegressor(verbose=1, feateng_steps=2, units=units)\n",
    "#és a legjobbak kiválasztása\n",
    "X_af = afreg.fit_transform(X, y)\n",
    "X_af.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#randomforest illesztése a kibővített adathalmazra, hogy feature importance-t alkalmazzunk rajta\n",
    "randforreg = RandomForestRegressor(random_state=13)\n",
    "randforreg.fit(X_af, y)\n",
    "#feature importance alkalmazása és ábrázolása\n",
    "importances = randforreg.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.bar(range(X_af.shape[1]), importances[indices])\n",
    "plt.xticks(range(X_af.shape[1]), indices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = []\n",
    "columns=[]\n",
    "#kiválasztom a legfontosabb featureket\n",
    "for i,v in enumerate(importances):\n",
    "    if v > 0.04:\n",
    "        select.append(i)\n",
    "for i in select:\n",
    "    columns.append(X_af.columns[i])\n",
    "#leszúkitem a fontos featurekkel az adathalmazt\n",
    "XS_af = X_af[columns]\n",
    "XS_af.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adathalmaz tesztelése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsname='boston'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "best params: {'alpha': 0.01}\n",
      "best score: -14.847513595970847\n",
      "MSE on training data: 13.885690733375325\n",
      "MSE on test data: 14.937686522928825\n",
      "R^2 on training data: 0.8366491819488221\n",
      "R^2 on test data: 0.8171872181735931\n"
     ]
    }
   ],
   "source": [
    "#Ridge regresszió\n",
    "print(\"####\", dsname)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "rreg = Ridge()\n",
    "params = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000., 25000., 50000., 100000.]}\n",
    "rreg = test_model(XS_af, y, rreg, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "best params: {'C': 100.0}\n",
      "best score: -12.880297038563384\n",
      "MSE on training data: 4.860078921462448\n",
      "MSE on test data: 11.65474644037807\n",
      "R^2 on training data: 0.9428261882784136\n",
      "R^2 on test data: 0.8573650200132078\n"
     ]
    }
   ],
   "source": [
    "#SVR regresszió\n",
    "print(\"####\", dsname)\n",
    "svr = SVR(gamma=\"scale\")\n",
    "params = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "svr = test_model(XS_af, y, svr, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "best params: {'min_samples_leaf': 0.01}\n",
      "best score: -21.444576255728133\n",
      "MSE on training data: 12.847390898639864\n",
      "MSE on test data: 16.35898987235349\n",
      "R^2 on training data: 0.8488637077252588\n",
      "R^2 on test data: 0.7997927964384283\n"
     ]
    }
   ],
   "source": [
    "#random forest regresszió\n",
    "print(\"####\", dsname)\n",
    "rforest = RandomForestRegressor(n_estimators=100, random_state=13)\n",
    "params = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "rforest = test_model(XS_af, y, rforest, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 28 features.\n",
      "[AutoFeat] With 404 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 20 transformed features from 4 original features - done.\n",
      "[feateng] Generated altogether 20 new features in 1 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 10 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 7 features after 5 feature selection runs\n",
      "[featsel] 7 features after correlation filtering\n",
      "[featsel] 6 features after noise filtering\n",
      "[AutoFeat] Computing 2 new features.\n",
      "[AutoFeat]     2/    2 new features ...done.\n",
      "[AutoFeat] Final dataframe with 6 feature columns (2 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "46.81323864421262\n",
      "22.730499 * 1/x012\n",
      "-4.136469 * x005\n",
      "-0.732779 * x012\n",
      "0.011751 * exp(x005)\n",
      "0.000166 * x012**3\n",
      "[AutoFeat] Final score: 0.7758\n",
      "[AutoFeat] Computing 2 new features.\n",
      "[AutoFeat]     2/    2 new features ...done.\n",
      "autofeat new features: 2\n",
      "autofeat MSE on training data: 19.0619246664332\n",
      "autofeat MSE on test data: 17.49683217389327\n",
      "autofeat R^2 on training data: 0.7757561328650667\n",
      "autofeat R^2 on test data: 0.7858674729885783\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.1}\n",
      "best score: -21.662669353062235\n",
      "MSE on training data: 19.08188684817668\n",
      "MSE on test data: 17.35351687723171\n",
      "R^2 on training data: 0.7755212984027058\n",
      "R^2 on test data: 0.7876214171499288\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.01}\n",
      "best score: -21.459659788691248\n",
      "MSE on training data: 12.848230185418252\n",
      "MSE on test data: 16.359607935138012\n",
      "R^2 on training data: 0.8488538343826602\n",
      "R^2 on test data: 0.7997852323637097\n",
      "# SVR\n",
      "best params: {'C': 25.0}\n",
      "best score: -22.732917635832596\n",
      "MSE on training data: 15.766360377764961\n",
      "MSE on test data: 15.606171093013701\n",
      "R^2 on training data: 0.8145250448933521\n",
      "R^2 on test data: 0.8090060635029779\n"
     ]
    }
   ],
   "source": [
    "#autofeat regresszió 1 steppel\n",
    "print(\"####\", dsname)\n",
    "test_autofeat(XS_af, y, units, feateng_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 406 features.\n",
      "[AutoFeat] With 404 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 20 transformed features from 4 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 260 feature combinations from 276 original feature tuples - done.\n",
      "[feateng] Generated altogether 286 new features in 2 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 87 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 16 features after 5 feature selection runs\n",
      "[featsel] 12 features after correlation filtering\n",
      "[featsel] 8 features after noise filtering\n",
      "[AutoFeat] Computing 6 new features.\n",
      "[AutoFeat]     6/    6 new features ...done.\n",
      "[AutoFeat] Final dataframe with 10 feature columns (6 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "18.312462538460736\n",
      "144566.382354 * x1x012**3/expx005\n",
      "-0.164340 * x012\n",
      "0.012969 * x005**3*sqrt(x1x012)\n",
      "0.010930 * exp(x005)\n",
      "-0.000027 * expx005*x012**2\n",
      "[AutoFeat] Final score: 0.8006\n",
      "[AutoFeat] Computing 6 new features.\n",
      "[AutoFeat]     6/    6 new features ...done.\n",
      "autofeat new features: 6\n",
      "autofeat MSE on training data: 16.95387709129392\n",
      "autofeat MSE on test data: 18.314009255035664\n",
      "autofeat R^2 on training data: 0.8005551365662027\n",
      "autofeat R^2 on test data: 0.7758665658722643\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -21.270569422095132\n",
      "MSE on training data: 17.53640097225888\n",
      "MSE on test data: 17.93812722256654\n",
      "R^2 on training data: 0.7937023444136841\n",
      "R^2 on test data: 0.7804667454173879\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.01}\n",
      "best score: -22.417051044488353\n",
      "MSE on training data: 12.127762359009237\n",
      "MSE on test data: 16.365862924620373\n",
      "R^2 on training data: 0.8573293946614579\n",
      "R^2 on test data: 0.7997086815520555\n",
      "# SVR\n",
      "best params: {'C': 25.0}\n",
      "best score: -23.539650265815542\n",
      "MSE on training data: 15.625956347146468\n",
      "MSE on test data: 16.26521226293\n",
      "R^2 on training data: 0.8161767533822994\n",
      "R^2 on test data: 0.800940480561095\n"
     ]
    }
   ],
   "source": [
    "#autofeat regresszió 2 steppel\n",
    "print(\"####\", dsname)\n",
    "test_autofeat(XS_af, y, units, feateng_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 9478 features.\n",
      "[AutoFeat] With 404 data points this new feature matrix would use about 0.02 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 20 transformed features from 4 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 1038 feature combinations from 276 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 4508 transformed features from 1038 original features - done.\n",
      "[feateng] Generated altogether 5961 new features in 3 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 2563 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 15 features after 5 feature selection runs\n",
      "[featsel] 9 features after correlation filtering\n",
      "[featsel] 5 features after noise filtering\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "[AutoFeat] Final dataframe with 9 feature columns (5 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "14.02889080874806\n",
      "87859.864119 * x1x012**3/expx005\n",
      "1.882352 * log(expx005**2*x1x012**3)\n",
      "-0.124282 * 1/(exp(x1x012) - log(x012))\n",
      "0.049043 * 1/(-x1x012**2 + 1/x005)\n",
      "-0.026320 * 1/(x1x012**2 + 1/expx005)\n",
      "[AutoFeat] Final score: 0.7544\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "autofeat new features: 5\n",
      "autofeat MSE on training data: 20.873234011752725\n",
      "autofeat MSE on test data: 77.9247714630089\n",
      "autofeat R^2 on training data: 0.754447948131374\n",
      "autofeat R^2 on test data: 0.04632861169813007\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 25.0}\n",
      "best score: -24.179828338314902\n",
      "MSE on training data: 20.917875918769727\n",
      "MSE on test data: 72.07221554520571\n",
      "R^2 on training data: 0.753922782176679\n",
      "R^2 on test data: 0.11795429660495349\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.01}\n",
      "best score: -22.880867635392782\n",
      "MSE on training data: 12.047479662998114\n",
      "MSE on test data: 17.752527903989407\n",
      "R^2 on training data: 0.8582738377086626\n",
      "R^2 on test data: 0.7827381766515412\n",
      "# SVR\n",
      "best params: {'C': 10.0}\n",
      "best score: -23.74612394276473\n",
      "MSE on training data: 16.188902899092813\n",
      "MSE on test data: 18.91820158717605\n",
      "R^2 on training data: 0.809554268297096\n",
      "R^2 on test data: 0.7684722427401498\n"
     ]
    }
   ],
   "source": [
    "#autofeat regresszió 3 steppel\n",
    "print(\"####\", dsname)\n",
    "test_autofeat(XS_af, y, units, feateng_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
