{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_wine, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from autofeat import AutoFeatClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "np.seterr(divide = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same interface for loading all datasets\n",
    "def load_classification_dataset(name):\n",
    "    # load one of the datasets as X and y\n",
    "    units = {}\n",
    "    if name == \"iris\":\n",
    "        # sklearn iris housing dataset\n",
    "        X, y = load_iris(True)\n",
    "\n",
    "    elif name == \"wine\":\n",
    "        # sklearn wine dataset\n",
    "        X, y = load_wine(True)\n",
    "    \n",
    "    elif name == \"breast_cancer\":\n",
    "        # sklearn breast_cancer dataset\n",
    "        X, y = load_breast_cancer(True)\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown dataset %r\" % name)\n",
    "    return np.array(X, dtype=float), np.array(y, dtype=float), units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X, y, model, param_grid):\n",
    "    # load data\n",
    "    #X, y, _ = load_classification_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    \n",
    "    if model.__class__.__name__ == \"SVC\":\n",
    "        sscaler = StandardScaler()\n",
    "        X_train = sscaler.fit_transform(X_train)\n",
    "        X_test = sscaler.transform(X_test)\n",
    "    \n",
    "    # train model on train split incl cross-validation for parameter selection\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gsmodel = GridSearchCV(model, param_grid, cv=5)\n",
    "        gsmodel.fit(X_train, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"Acc. on training data:\", accuracy_score(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"Acc. on test data:\", accuracy_score(y_test, gsmodel.predict(X_test)))\n",
    "    return gsmodel.best_estimator_\n",
    "\n",
    "def test_autofeat(X, y, units, feateng_steps=2):\n",
    "    # load data\n",
    "    #X, y, units = load_classification_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    # run autofeat\n",
    "    afclas = AutoFeatClassifier(verbose=1, feateng_steps=feateng_steps, units=units)\n",
    "    # fit autofeat on less data, otherwise ridge reg model with xval will overfit on new features\n",
    "    X_train_tr = afclas.fit_transform(X_train, y_train)\n",
    "    X_test_tr = afclas.transform(X_test)\n",
    "    print(\"autofeat new features:\", len(afclas.new_feat_cols_))\n",
    "    print(\"autofeat Acc. on training data:\", accuracy_score(y_train, afclas.predict(X_train_tr)))\n",
    "    print(\"autofeat Acc. on test data:\", accuracy_score(y_test, afclas.predict(X_test_tr)))\n",
    "    \n",
    "    # train rreg on transformed train split incl cross-validation for parameter selection\n",
    "    print(\"# Logistic Regression\")\n",
    "    rreg = LogisticRegression(class_weight=\"balanced\")\n",
    "    param_grid = {\"C\": np.logspace(-4, 4, 10)}\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gsmodel = GridSearchCV(rreg, param_grid, cv=5)\n",
    "        gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"Acc. on training data:\", accuracy_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"Acc. on test data:\", accuracy_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    \n",
    "    print(\"# Random Forest\")\n",
    "    rforest = RandomForestClassifier(n_estimators=100, random_state=13)\n",
    "    param_grid = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "    gsmodel = GridSearchCV(rforest, param_grid, cv=5)\n",
    "    gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"Acc. on training data:\", accuracy_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"Acc. on test data:\", accuracy_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    \n",
    "    print(\"# SVC\")\n",
    "    svc = SVC(gamma=\"scale\", class_weight=\"balanced\")\n",
    "    param_grid = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "    sscaler = StandardScaler()\n",
    "    X_train_tr = sscaler.fit_transform(X_train_tr)\n",
    "    X_test_tr = sscaler.transform(X_test_tr)\n",
    "    gsmodel = GridSearchCV(svc, param_grid, cv=5)\n",
    "    gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"Acc. on training data:\", accuracy_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"Acc. on test data:\", accuracy_score(y_test, gsmodel.predict(X_test_tr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### wine\n",
      "(178, 13) [0. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "dsname = 'wine'\n",
    "print(\"####\", dsname)\n",
    "X, y, _ = load_classification_dataset(dsname)\n",
    "print(X.shape, np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#randomforest illesztése a kibővített adathalmazra, hogy feature importance-t alkalmazzunk rajta\n",
    "randforclas = RandomForestClassifier(random_state=13)\n",
    "randforclas.fit(X, y)\n",
    "#feature importance alkalmazása és ábrázolása\n",
    "importances = randforclas.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.bar(range(X.shape[1]), importances[indices])\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = []\n",
    "#kiválasztom a legfontosabb featureket\n",
    "for i,v in enumerate(importances):\n",
    "    if v > 0.075:\n",
    "        select.append(i)\n",
    "XS = X[:,select]\n",
    "XS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### wine\n",
      "best params: {'C': 0.046415888336127774}\n",
      "best score: 0.9293103448275863\n",
      "Acc. on training data: 0.9366197183098591\n",
      "Acc. on test data: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "rreg = LogisticRegression(class_weight=\"balanced\")\n",
    "params = {\"C\": np.logspace(-4, 4, 10)}\n",
    "rreg = test_model(XS, y, rreg, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### wine\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: 0.9576354679802955\n",
      "Acc. on training data: 1.0\n",
      "Acc. on test data: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "rforest = RandomForestClassifier(n_estimators=100, random_state=13)\n",
    "params = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "rforest = test_model(XS, y, rforest, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### wine\n",
      "best params: {'C': 1.0}\n",
      "best score: 0.9573891625615764\n",
      "Acc. on training data: 0.9788732394366197\n",
      "Acc. on test data: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "svc = SVC(gamma=\"scale\", class_weight=\"balanced\")\n",
    "params = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "svc = test_model(XS, y, svc, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### wine\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 35 features.\n",
      "[AutoFeat] With 142 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 27 transformed features from 5 original features - done.\n",
      "[feateng] Generated altogether 27 new features in 1 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 8 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 7 features after 5 feature selection runs\n",
      "[featsel] 7 features after correlation filtering\n",
      "[featsel] 5 features after noise filtering\n",
      "[AutoFeat] Computing 2 new features.\n",
      "[AutoFeat]     2/    2 new features ...done.\n",
      "[AutoFeat] Final dataframe with 7 feature columns (2 new).\n",
      "[AutoFeat] Training final classification model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "[-12.06549445  12.16456145  -0.09906699]\n",
      "7.424816 * 1/x002\n",
      "4.318069 * 1/x001\n",
      "2.355449 * x003\n",
      "0.998619 * x000\n",
      "0.006207 * x004\n",
      "[AutoFeat] Final score: 0.9789\n",
      "[AutoFeat] Computing 2 new features.\n",
      "[AutoFeat]     2/    2 new features ...done.\n",
      "autofeat new features: 2\n",
      "autofeat Acc. on training data: 0.9788732394366197\n",
      "autofeat Acc. on test data: 1.0\n",
      "# Logistic Regression\n",
      "best params: {'C': 0.046415888336127774}\n",
      "best score: 0.9293103448275863\n",
      "Acc. on training data: 0.9295774647887324\n",
      "Acc. on test data: 1.0\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: 0.9578817733990148\n",
      "Acc. on training data: 1.0\n",
      "Acc. on test data: 0.9722222222222222\n",
      "# SVC\n",
      "best params: {'C': 1.0}\n",
      "best score: 0.9716748768472907\n",
      "Acc. on training data: 0.9859154929577465\n",
      "Acc. on test data: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "test_autofeat(XS, y, _, feateng_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### wine\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 630 features.\n",
      "[AutoFeat] With 142 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 27 transformed features from 5 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 476 feature combinations from 496 original feature tuples - done.\n",
      "[feateng] Generated altogether 504 new features in 2 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 201 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 20 features after 5 feature selection runs\n",
      "[featsel] 11 features after correlation filtering\n",
      "[featsel] 10 features after noise filtering\n",
      "[AutoFeat] Computing 9 new features.\n",
      "[AutoFeat]     9/    9 new features ...done.\n",
      "[AutoFeat] Final dataframe with 14 feature columns (9 new).\n",
      "[AutoFeat] Training final classification model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "[-1.42170939  1.42721171 -0.00550232]\n",
      "3.415851 * x000\n",
      "0.764354 * x000**2/x003\n",
      "0.654831 * x000*log(x004)\n",
      "0.353608 * log(x002)/x001\n",
      "0.191390 * sqrt(x003)*sqrt(x004)\n",
      "0.174919 * log(x003)/x003\n",
      "0.099665 * x000**3*sqrt(x001)\n",
      "0.035927 * x000**3*log(x002)\n",
      "0.008764 * sqrt(x001)/x004\n",
      "0.001464 * 1/x002\n",
      "[AutoFeat] Final score: 0.9859\n",
      "[AutoFeat] Computing 9 new features.\n",
      "[AutoFeat]     9/    9 new features ...done.\n",
      "autofeat new features: 9\n",
      "autofeat Acc. on training data: 0.9859154929577465\n",
      "autofeat Acc. on test data: 1.0\n",
      "# Logistic Regression\n",
      "best params: {'C': 0.005994842503189409}\n",
      "best score: 0.9433497536945813\n",
      "Acc. on training data: 0.971830985915493\n",
      "Acc. on test data: 1.0\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: 0.9716748768472907\n",
      "Acc. on training data: 1.0\n",
      "Acc. on test data: 1.0\n",
      "# SVC\n",
      "best params: {'C': 10.0}\n",
      "best score: 0.9719211822660098\n",
      "Acc. on training data: 0.9929577464788732\n",
      "Acc. on test data: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "test_autofeat(XS, y, _, feateng_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### wine\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 14910 features.\n",
      "[AutoFeat] With 142 data points this new feature matrix would use about 0.01 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 27 transformed features from 5 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 1904 feature combinations from 496 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 8263 transformed features from 1904 original features - done.\n",
      "[feateng] Generated altogether 10855 new features in 3 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 4314 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 40 features after 5 feature selection runs\n",
      "[featsel] 11 features after correlation filtering\n",
      "[featsel] 6 features after noise filtering\n",
      "[AutoFeat] Computing 6 new features.\n",
      "[AutoFeat]     6/    6 new features ...done.\n",
      "[AutoFeat] Final dataframe with 11 feature columns (6 new).\n",
      "[AutoFeat] Training final classification model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "[-7.21146766e-06  8.32870714e-06 -1.11723947e-06]\n",
      "0.010234 * x000**3*sqrt(x001)\n",
      "0.004347 * x004*log(x003)\n",
      "0.003707 * x000**3*log(x002)\n",
      "0.000050 * log(x002*x004)\n",
      "[AutoFeat] Final score: 0.8169\n",
      "[AutoFeat] Computing 6 new features.\n",
      "[AutoFeat]     6/    6 new features ...done.\n",
      "autofeat new features: 6\n",
      "autofeat Acc. on training data: 0.8169014084507042\n",
      "autofeat Acc. on test data: 0.8888888888888888\n",
      "# Logistic Regression\n",
      "best params: {'C': 0.000774263682681127}\n",
      "best score: 0.8017241379310345\n",
      "Acc. on training data: 0.8169014084507042\n",
      "Acc. on test data: 0.8888888888888888\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: 0.9716748768472907\n",
      "Acc. on training data: 1.0\n",
      "Acc. on test data: 1.0\n",
      "# SVC\n",
      "best params: {'C': 1.0}\n",
      "best score: 0.9716748768472907\n",
      "Acc. on training data: 0.9859154929577465\n",
      "Acc. on test data: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "test_autofeat(XS, y, _, feateng_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
