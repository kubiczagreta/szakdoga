{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from autofeat import AutoFeatRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "np.seterr(divide = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same interface for loading all datasets - adapt the datapath\n",
    "# to where you've downloaded (and renamed) the datasets\n",
    "def load_regression_dataset(name, datapath=\"../datasets/regression/\"):\n",
    "    # load one of the datasets as X and y (and possibly units)\n",
    "    units = {}\n",
    "    if name == \"boston\":\n",
    "        # sklearn boston housing dataset\n",
    "        X, y = load_boston(True)\n",
    "\n",
    "    elif name == \"diabetes\":\n",
    "        # sklearn diabetes dataset\n",
    "        X, y = load_diabetes(True)\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown dataset %r\" % name)\n",
    "    return np.array(X, dtype=float), np.array(y, dtype=float), units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X, y, model, param_grid):\n",
    "    # load data\n",
    "    #X, y, _ = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    \n",
    "    if model.__class__.__name__ == \"SVR\":\n",
    "        sscaler = StandardScaler()\n",
    "        X_train = sscaler.fit_transform(X_train)\n",
    "        X_test = sscaler.transform(X_test)\n",
    "        \n",
    "    # train model on train split incl cross-validation for parameter selection\n",
    "    gsmodel = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    gsmodel.fit(X_train, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test)))\n",
    "    return gsmodel.best_estimator_\n",
    "\n",
    "def test_autofeat(X, y, units, feateng_steps):\n",
    "    # load data\n",
    "    #X, y, units = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    # run autofeat\n",
    "    afreg = AutoFeatRegressor(verbose=1, feateng_steps=feateng_steps, units=units)\n",
    "    # fit autofeat on less data, otherwise ridge reg model with xval will overfit on new features\n",
    "    X_train_tr = afreg.fit_transform(X_train, y_train)\n",
    "    X_test_tr = afreg.transform(X_test)\n",
    "    print(\"autofeat new features:\", len(afreg.new_feat_cols_))\n",
    "    print(\"autofeat MSE on training data:\", mean_squared_error(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat MSE on test data:\", mean_squared_error(y_test, afreg.predict(X_test_tr)))\n",
    "    print(\"autofeat R^2 on training data:\", r2_score(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat R^2 on test data:\", r2_score(y_test, afreg.predict(X_test_tr)))\n",
    "    \n",
    "    \n",
    "    # train rreg on transformed train split incl cross-validation for parameter selection\n",
    "    print(\"# Ridge Regression\")\n",
    "    rreg = Ridge()\n",
    "    param_grid = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000.]}\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gsmodel = GridSearchCV(rreg, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "        gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    \n",
    "    \n",
    "    print(\"# Random Forest\")\n",
    "    rforest = RandomForestRegressor(n_estimators=100, random_state=13)\n",
    "    param_grid = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "    gsmodel = GridSearchCV(rforest, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    \n",
    "    \n",
    "    print(\"# SVR\")\n",
    "    svr = SVR(gamma=\"scale\")\n",
    "    param_grid = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "    sscaler = StandardScaler()\n",
    "    X_train_tr = sscaler.fit_transform(X_train_tr)\n",
    "    X_test_tr = sscaler.transform(X_test_tr)\n",
    "\n",
    "    gsmodel = GridSearchCV(svr, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "(442, 10)\n"
     ]
    }
   ],
   "source": [
    "dsname = 'diabetes'\n",
    "print(\"####\", dsname)\n",
    "X, y, _ = load_regression_dataset(dsname)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#randomforest illesztése a kibővített adathalmazra, hogy feature importance-t alkalmazzunk rajta\n",
    "randforreg = RandomForestRegressor(random_state=13)\n",
    "randforreg.fit(X, y)\n",
    "#feature importance alkalmazása és ábrázolása\n",
    "importances = randforreg.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.bar(range(X.shape[1]), importances[indices])\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = []\n",
    "#kiválasztom a legfontosabb featureket\n",
    "for i,v in enumerate(importances):\n",
    "    if v > 0.1:\n",
    "        select.append(i)\n",
    "XS = X[:,select]\n",
    "XS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "best params: {'alpha': 0.01}\n",
      "best score: -3277.0593286152507\n",
      "MSE on training data: 3188.356809815925\n",
      "MSE on test data: 3279.3426775682083\n",
      "R^2 on training data: 0.48095707165436086\n",
      "R^2 on test data: 0.3514223582171676\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "rreg = Ridge()\n",
    "params = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000., 25000., 50000., 100000.]}\n",
    "rreg = test_model(XS, y, rreg, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3375.0127427729058\n",
      "MSE on training data: 2777.476364452161\n",
      "MSE on test data: 3324.3099104024823\n",
      "R^2 on training data: 0.5478456297056415\n",
      "R^2 on test data: 0.3425288863550626\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "rforest = RandomForestRegressor(n_estimators=100, random_state=13)\n",
    "params = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "rforest = test_model(XS, y, rforest, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "best params: {'C': 50.0}\n",
      "best score: -3357.35932007566\n",
      "MSE on training data: 3054.081824674205\n",
      "MSE on test data: 3404.5225608023215\n",
      "R^2 on training data: 0.502816131241719\n",
      "R^2 on test data: 0.32666469137680043\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "svr = SVR(gamma=\"scale\")\n",
    "params = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "svr = test_model(XS, y, svr, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 14 features.\n",
      "[AutoFeat] With 353 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 10 transformed features from 2 original features - done.\n",
      "[feateng] Generated altogether 10 new features in 1 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 8 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 4 features after 5 feature selection runs\n",
      "[featsel] 4 features after correlation filtering\n",
      "[featsel] 4 features after noise filtering\n",
      "[AutoFeat] Computing 2 new features.\n",
      "[AutoFeat]     2/    2 new features ...done.\n",
      "[AutoFeat] Final dataframe with 4 feature columns (2 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "154.22297696177802\n",
      "-27796.455789 * x001**3\n",
      "813.759319 * x001\n",
      "660.920427 * x000\n",
      "0.019230 * 1/x001\n",
      "[AutoFeat] Final score: 0.4951\n",
      "[AutoFeat] Computing 2 new features.\n",
      "[AutoFeat]     2/    2 new features ...done.\n",
      "autofeat new features: 2\n",
      "autofeat MSE on training data: 3101.3306256937994\n",
      "autofeat MSE on test data: 3283.7540709552095\n",
      "autofeat R^2 on training data: 0.49512434594136356\n",
      "autofeat R^2 on test data: 0.350549887298075\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -3219.0763323775113\n",
      "MSE on training data: 3106.143280129417\n",
      "MSE on test data: 3288.3566859054768\n",
      "R^2 on training data: 0.4943408783433556\n",
      "R^2 on test data: 0.3496395972052164\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3367.2177453410413\n",
      "MSE on training data: 2769.138361762169\n",
      "MSE on test data: 3332.2458551953646\n",
      "R^2 on training data: 0.5492030001603672\n",
      "R^2 on test data: 0.3409593412159425\n",
      "# SVR\n",
      "best params: {'C': 50.0}\n",
      "best score: -3468.0366248794853\n",
      "MSE on training data: 3047.086134997569\n",
      "MSE on test data: 3403.7120779306865\n",
      "R^2 on training data: 0.5039549822148535\n",
      "R^2 on test data: 0.3268249860215736\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "test_autofeat(XS, y, _, feateng_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 105 features.\n",
      "[AutoFeat] With 353 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 10 transformed features from 2 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 54 feature combinations from 66 original feature tuples - done.\n",
      "[feateng] Generated altogether 69 new features in 2 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 38 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 4 features after 5 feature selection runs\n",
      "[featsel] 4 features after correlation filtering\n",
      "[featsel] 4 features after noise filtering\n",
      "[AutoFeat] Computing 3 new features.\n",
      "[AutoFeat]     3/    3 new features ...done.\n",
      "[AutoFeat] Final dataframe with 5 feature columns (3 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-229.39397071145126\n",
      "382.426390 * exp(x000)*exp(x001)\n",
      "226.710685 * x000\n",
      "14.360619 * Abs(x001)/x001\n",
      "3.660307 * Abs(x000)/x000\n",
      "[AutoFeat] Final score: 0.4951\n",
      "[AutoFeat] Computing 3 new features.\n",
      "[AutoFeat]     3/    3 new features ...done.\n",
      "autofeat new features: 3\n",
      "autofeat MSE on training data: 3101.410664155817\n",
      "autofeat MSE on test data: 3263.0919974248973\n",
      "autofeat R^2 on training data: 0.4951113162209828\n",
      "autofeat R^2 on test data: 0.3546363644498226\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.01}\n",
      "best score: -3237.827755833059\n",
      "MSE on training data: 3100.2624374605016\n",
      "MSE on test data: 3262.5647608883073\n",
      "R^2 on training data: 0.4952982397624466\n",
      "R^2 on test data: 0.3547406395632172\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3225.7263163479006\n",
      "MSE on training data: 2637.736943838463\n",
      "MSE on test data: 3468.724739931382\n",
      "R^2 on training data: 0.5705942624362562\n",
      "R^2 on test data: 0.3139669949080304\n",
      "# SVR\n",
      "best params: {'C': 50.0}\n",
      "best score: -3341.5239425500768\n",
      "MSE on training data: 3047.0014489212194\n",
      "MSE on test data: 3584.9465602463456\n",
      "R^2 on training data: 0.5039687685354193\n",
      "R^2 on test data: 0.29098102437826423\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "test_autofeat(XS, y, _, feateng_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 2289 features.\n",
      "[AutoFeat] With 353 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 10 transformed features from 2 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 252 feature combinations from 66 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 1084 transformed features from 252 original features - done.\n",
      "[feateng] Generated altogether 1442 new features in 3 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 687 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 10 features after 5 feature selection runs\n",
      "[featsel] 10 features after correlation filtering\n",
      "[featsel] 6 features after noise filtering\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "[AutoFeat] Final dataframe with 7 feature columns (5 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-186.13183152918944\n",
      "338.043776 * exp(x000)*exp(x001)\n",
      "330.118712 * x000\n",
      "13.102807 * Abs(x001)/x001\n",
      "0.960673 * x001/Abs(x000)\n",
      "0.204880 * Abs(Abs(x000)/x001)\n",
      "0.015757 * 1/(x000 + x001**2)\n",
      "[AutoFeat] Final score: 0.5147\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "autofeat new features: 5\n",
      "autofeat MSE on training data: 2980.9271977175185\n",
      "autofeat MSE on test data: 3856.615703829376\n",
      "autofeat R^2 on training data: 0.514725209824371\n",
      "autofeat R^2 on test data: 0.23725119196535416\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.1}\n",
      "best score: -3112.8249002731072\n",
      "MSE on training data: 2984.501126023328\n",
      "MSE on test data: 3858.832461681285\n",
      "R^2 on training data: 0.5141433984637875\n",
      "R^2 on test data: 0.23681276886616742\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3213.513276026203\n",
      "MSE on training data: 2578.410591673131\n",
      "MSE on test data: 3506.3379232827906\n",
      "R^2 on training data: 0.5802521913923746\n",
      "R^2 on test data: 0.3065279828386689\n",
      "# SVR\n",
      "best params: {'C': 100.0}\n",
      "best score: -3443.251373255343\n",
      "MSE on training data: 2920.705252411929\n",
      "MSE on test data: 3771.0979981027253\n",
      "R^2 on training data: 0.5245289352875473\n",
      "R^2 on test data: 0.2541646033908411\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "test_autofeat(XS, y, _, feateng_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
