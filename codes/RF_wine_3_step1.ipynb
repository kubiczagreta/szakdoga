{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_wine, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from autofeat import AutoFeatClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "np.seterr(divide = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same interface for loading all datasets\n",
    "def load_classification_dataset(name):\n",
    "    # load one of the datasets as X and y\n",
    "    units = {}\n",
    "    if name == \"iris\":\n",
    "        # sklearn iris housing dataset\n",
    "        X, y = load_iris(True)\n",
    "\n",
    "    elif name == \"wine\":\n",
    "        # sklearn wine dataset\n",
    "        X, y = load_wine(True)\n",
    "    \n",
    "    elif name == \"breast_cancer\":\n",
    "        # sklearn breast_cancer dataset\n",
    "        X, y = load_breast_cancer(True)\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown dataset %r\" % name)\n",
    "    return np.array(X, dtype=float), np.array(y, dtype=float), units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X, y, model, param_grid):\n",
    "    # load data\n",
    "    #X, y, _ = load_classification_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    \n",
    "    if model.__class__.__name__ == \"SVC\":\n",
    "        sscaler = StandardScaler()\n",
    "        X_train = sscaler.fit_transform(X_train)\n",
    "        X_test = sscaler.transform(X_test)\n",
    "    \n",
    "    # train model on train split incl cross-validation for parameter selection\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gsmodel = GridSearchCV(model, param_grid, cv=5)\n",
    "        gsmodel.fit(X_train, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"Acc. on training data:\", accuracy_score(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"Acc. on test data:\", accuracy_score(y_test, gsmodel.predict(X_test)))\n",
    "    return gsmodel.best_estimator_\n",
    "\n",
    "def test_autofeat(X, y, units, feateng_steps=2):\n",
    "    # load data\n",
    "    #X, y, units = load_classification_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    # run autofeat\n",
    "    afclas = AutoFeatClassifier(verbose=1, feateng_steps=feateng_steps, units=units)\n",
    "    # fit autofeat on less data, otherwise ridge reg model with xval will overfit on new features\n",
    "    X_train_tr = afclas.fit_transform(X_train, y_train)\n",
    "    X_test_tr = afclas.transform(X_test)\n",
    "    print(\"autofeat new features:\", len(afclas.new_feat_cols_))\n",
    "    print(\"autofeat Acc. on training data:\", accuracy_score(y_train, afclas.predict(X_train_tr)))\n",
    "    print(\"autofeat Acc. on test data:\", accuracy_score(y_test, afclas.predict(X_test_tr)))\n",
    "    \n",
    "    # train rreg on transformed train split incl cross-validation for parameter selection\n",
    "    print(\"# Logistic Regression\")\n",
    "    rreg = LogisticRegression(class_weight=\"balanced\")\n",
    "    param_grid = {\"C\": np.logspace(-4, 4, 10)}\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gsmodel = GridSearchCV(rreg, param_grid, cv=5)\n",
    "        gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"Acc. on training data:\", accuracy_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"Acc. on test data:\", accuracy_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    \n",
    "    print(\"# Random Forest\")\n",
    "    rforest = RandomForestClassifier(n_estimators=100, random_state=13)\n",
    "    param_grid = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "    gsmodel = GridSearchCV(rforest, param_grid, cv=5)\n",
    "    gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"Acc. on training data:\", accuracy_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"Acc. on test data:\", accuracy_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    \n",
    "    print(\"# SVC\")\n",
    "    svc = SVC(gamma=\"scale\", class_weight=\"balanced\")\n",
    "    param_grid = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "    sscaler = StandardScaler()\n",
    "    X_train_tr = sscaler.fit_transform(X_train_tr)\n",
    "    X_test_tr = sscaler.transform(X_test_tr)\n",
    "    gsmodel = GridSearchCV(svc, param_grid, cv=5)\n",
    "    gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"Acc. on training data:\", accuracy_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"Acc. on test data:\", accuracy_score(y_test, gsmodel.predict(X_test_tr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### wine\n",
      "(178, 13) [0. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "dsname = 'wine'\n",
    "print(\"####\", dsname)\n",
    "X, y, units = load_classification_dataset(dsname)\n",
    "print(X.shape, np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoFeat] The 1 step feature engineering process could generate up to 91 features.\n",
      "[AutoFeat] With 178 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 73 transformed features from 13 original features - done.\n",
      "[feateng] Generated altogether 73 new features in 1 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 16 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 12 features after 5 feature selection runs\n",
      "[featsel] 12 features after correlation filtering\n",
      "[featsel] 9 features after noise filtering\n",
      "[AutoFeat] Computing 3 new features.\n",
      "[AutoFeat]     3/    3 new features ...done.\n",
      "[AutoFeat] Final dataframe with 16 feature columns (3 new).\n",
      "[AutoFeat] Training final classification model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "[-11.55960575  11.34511405   0.2144917 ]\n",
      "8.672914 * x011\n",
      "8.055782 * 1/x006\n",
      "7.930343 * 1/x001\n",
      "7.374707 * x010\n",
      "7.370275 * x002\n",
      "5.961489 * 1/x009\n",
      "1.301248 * x000\n",
      "0.775938 * x003\n",
      "0.021247 * x012\n",
      "[AutoFeat] Final score: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(178, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#új változók generálása autofeattel\n",
    "afclas = AutoFeatClassifier(verbose=1, feateng_steps=1, units=units)\n",
    "# fit autofeat on less data, otherwise ridge reg model with xval will overfit on new features\n",
    "X_af = afclas.fit_transform(X, y)\n",
    "X_af.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#randomforest illesztése a kibővített adathalmazra, hogy feature importance-t alkalmazzunk rajta\n",
    "randforclas = RandomForestClassifier(random_state=13)\n",
    "randforclas.fit(X_af, y)\n",
    "#feature importance alkalmazása és ábrázolása\n",
    "importances = randforclas.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.bar(range(X_af.shape[1]), importances[indices])\n",
    "plt.xticks(range(X_af.shape[1]), indices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = []\n",
    "columns=[]\n",
    "#kiválasztom a legfontosabb featureket\n",
    "for i,v in enumerate(importances):\n",
    "    if v > 0.05:\n",
    "        select.append(i)\n",
    "for i in select:\n",
    "    columns.append(X_af.columns[i])\n",
    "XS_af = X_af[columns]\n",
    "XS_af.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### wine\n",
      "best params: {'C': 0.005994842503189409}\n",
      "best score: 0.9295566502463055\n",
      "Acc. on training data: 0.9225352112676056\n",
      "Acc. on test data: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "rreg = LogisticRegression(class_weight=\"balanced\")\n",
    "params = {\"C\": np.logspace(-4, 4, 10)}\n",
    "rreg = test_model(XS_af, y, rreg, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### wine\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: 0.9862068965517242\n",
      "Acc. on training data: 1.0\n",
      "Acc. on test data: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "rforest = RandomForestClassifier(n_estimators=100, random_state=13)\n",
    "params = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "rforest = test_model(XS_af, y, rforest, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### wine\n",
      "best params: {'C': 1.0}\n",
      "best score: 0.9716748768472907\n",
      "Acc. on training data: 0.9859154929577465\n",
      "Acc. on test data: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "svc = SVC(gamma=\"scale\", class_weight=\"balanced\")\n",
    "params = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "svc = test_model(XS_af, y, svc, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### wine\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 56 features.\n",
      "[AutoFeat] With 142 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 43 transformed features from 8 original features - done.\n",
      "[feateng] Generated altogether 43 new features in 1 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 11 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 8 features after 5 feature selection runs\n",
      "[featsel] 8 features after correlation filtering\n",
      "[featsel] 6 features after noise filtering\n",
      "[AutoFeat] Final dataframe with 8 feature columns (0 new).\n",
      "[AutoFeat] Training final classification model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "[-21.79941469  25.10729954  -3.30788483]\n",
      "18.557402 * x010\n",
      "16.737226 * 1/x009\n",
      "14.210753 * 1/x006\n",
      "6.723664 * x011\n",
      "3.198230 * x000\n",
      "0.013913 * x012\n",
      "[AutoFeat] Final score: 1.0000\n",
      "autofeat new features: 0\n",
      "autofeat Acc. on training data: 1.0\n",
      "autofeat Acc. on test data: 1.0\n",
      "# Logistic Regression\n",
      "best params: {'C': 0.005994842503189409}\n",
      "best score: 0.9295566502463055\n",
      "Acc. on training data: 0.9225352112676056\n",
      "Acc. on test data: 1.0\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: 0.9862068965517242\n",
      "Acc. on training data: 1.0\n",
      "Acc. on test data: 1.0\n",
      "# SVC\n",
      "best params: {'C': 1.0}\n",
      "best score: 0.9716748768472907\n",
      "Acc. on training data: 0.9859154929577465\n",
      "Acc. on test data: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "test_autofeat(XS_af, y, units, feateng_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### wine\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 1596 features.\n",
      "[AutoFeat] With 142 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 43 transformed features from 8 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 1243 feature combinations from 1275 original feature tuples - done.\n",
      "[feateng] Generated altogether 1295 new features in 2 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 572 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 24 features after 5 feature selection runs\n",
      "[featsel] 16 features after correlation filtering\n",
      "[featsel] 13 features after noise filtering\n",
      "[AutoFeat] Computing 11 new features.\n",
      "[AutoFeat]    11/   11 new features ...done.\n",
      "[AutoFeat] Final dataframe with 19 feature columns (11 new).\n",
      "[AutoFeat] Training final classification model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "[-5.25521475e-01  5.25854064e-01 -3.32589184e-04]\n",
      "4.594747 * sqrt(x012)/x010\n",
      "2.811695 * x000*log(x012)\n",
      "1.653714 * x000\n",
      "0.444706 * sqrt(x1x006)*log(x009)\n",
      "0.381847 * x012/x010\n",
      "0.302232 * 1/(x010*x011)\n",
      "0.219242 * x012*log(x011)\n",
      "0.172364 * x000**3*sqrt(x006)\n",
      "0.148288 * log(x011)/x011\n",
      "0.114205 * x000**3*log(x009)\n",
      "0.071896 * sqrt(x010)/x000\n",
      "0.052118 * 1/x006\n",
      "0.002050 * x010/x012\n",
      "[AutoFeat] Final score: 1.0000\n",
      "[AutoFeat] Computing 11 new features.\n",
      "[AutoFeat]    11/   11 new features ...done.\n",
      "autofeat new features: 11\n",
      "autofeat Acc. on training data: 1.0\n",
      "autofeat Acc. on test data: 1.0\n",
      "# Logistic Regression\n",
      "best params: {'C': 1291.5496650148827}\n",
      "best score: 0.9366995073891626\n",
      "Acc. on training data: 0.9929577464788732\n",
      "Acc. on test data: 1.0\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: 0.9788177339901478\n",
      "Acc. on training data: 1.0\n",
      "Acc. on test data: 0.9722222222222222\n",
      "# SVC\n",
      "best params: {'C': 1.0}\n",
      "best score: 0.9928571428571429\n",
      "Acc. on training data: 0.9929577464788732\n",
      "Acc. on test data: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "test_autofeat(XS_af, y, units, feateng_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sokáig futna\n",
    "#print(\"####\", dsname)\n",
    "#test_autofeat(XS_af, y, units, feateng_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
