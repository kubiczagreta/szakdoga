{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from autofeat import AutoFeatRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "np.seterr(divide = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same interface for loading all datasets - adapt the datapath\n",
    "# to where you've downloaded (and renamed) the datasets\n",
    "def load_regression_dataset(name, datapath=\"../datasets/regression/\"):\n",
    "    # load one of the datasets as X and y (and possibly units)\n",
    "    units = {}\n",
    "    if name == \"boston\":\n",
    "        # sklearn boston housing dataset\n",
    "        X, y = load_boston(True)\n",
    "\n",
    "    elif name == \"diabetes\":\n",
    "        # sklearn diabetes dataset\n",
    "        X, y = load_diabetes(True)\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown dataset %r\" % name)\n",
    "    return np.array(X, dtype=float), np.array(y, dtype=float), units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X, y, model, param_grid):\n",
    "    # load data\n",
    "    #X, y, _ = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    \n",
    "    if model.__class__.__name__ == \"SVR\":\n",
    "        sscaler = StandardScaler()\n",
    "        X_train = sscaler.fit_transform(X_train)\n",
    "        X_test = sscaler.transform(X_test)\n",
    "        \n",
    "    # train model on train split incl cross-validation for parameter selection\n",
    "    gsmodel = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    gsmodel.fit(X_train, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test)))\n",
    "    return gsmodel.best_estimator_\n",
    "\n",
    "def test_autofeat(X, y, units, feateng_steps):\n",
    "    # load data\n",
    "    #X, y, units = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    # run autofeat\n",
    "    afreg = AutoFeatRegressor(verbose=1, feateng_steps=feateng_steps, units=units)\n",
    "    # fit autofeat on less data, otherwise ridge reg model with xval will overfit on new features\n",
    "    X_train_tr = afreg.fit_transform(X_train, y_train)\n",
    "    X_test_tr = afreg.transform(X_test)\n",
    "    print(\"autofeat new features:\", len(afreg.new_feat_cols_))\n",
    "    print(\"autofeat MSE on training data:\", mean_squared_error(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat MSE on test data:\", mean_squared_error(y_test, afreg.predict(X_test_tr)))\n",
    "    print(\"autofeat R^2 on training data:\", r2_score(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat R^2 on test data:\", r2_score(y_test, afreg.predict(X_test_tr)))\n",
    "    \n",
    "    \n",
    "    # train rreg on transformed train split incl cross-validation for parameter selection\n",
    "    print(\"# Ridge Regression\")\n",
    "    rreg = Ridge()\n",
    "    param_grid = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000.]}\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gsmodel = GridSearchCV(rreg, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "        gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    \n",
    "    \n",
    "    print(\"# Random Forest\")\n",
    "    rforest = RandomForestRegressor(n_estimators=100, random_state=13)\n",
    "    param_grid = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "    gsmodel = GridSearchCV(rforest, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    \n",
    "    \n",
    "    print(\"# SVR\")\n",
    "    svr = SVR(gamma=\"scale\")\n",
    "    param_grid = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "    sscaler = StandardScaler()\n",
    "    X_train_tr = sscaler.fit_transform(X_train_tr)\n",
    "    X_test_tr = sscaler.transform(X_test_tr)\n",
    "\n",
    "    gsmodel = GridSearchCV(svr, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "dsname = 'boston'\n",
    "print(\"####\", dsname)\n",
    "X, y, _ = load_regression_dataset(dsname)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#randomforest illesztése a kibővített adathalmazra, hogy feature importance-t alkalmazzunk rajta\n",
    "randforreg = RandomForestRegressor(random_state=13)\n",
    "randforreg.fit(X, y)\n",
    "#feature importance alkalmazása és ábrázolása\n",
    "importances = randforreg.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.bar(range(X.shape[1]), importances[indices])\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = []\n",
    "#kiválasztom a legfontosabb featureket\n",
    "for i,v in enumerate(importances):\n",
    "    if v > 0.1:\n",
    "        select.append(i)\n",
    "XS = X[:,select]\n",
    "XS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "best params: {'alpha': 5.0}\n",
      "best score: -31.80170919814065\n",
      "MSE on training data: 30.815779096445446\n",
      "MSE on test data: 29.66177361553532\n",
      "R^2 on training data: 0.6374841683467953\n",
      "R^2 on test data: 0.6369885430225286\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "rreg = Ridge()\n",
    "params = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000., 25000., 50000., 100000.]}\n",
    "rreg = test_model(XS, y, rreg, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "best params: {'min_samples_leaf': 0.01}\n",
      "best score: -21.44492074645934\n",
      "MSE on training data: 12.846832762361814\n",
      "MSE on test data: 16.362622660791043\n",
      "R^2 on training data: 0.8488702736224369\n",
      "R^2 on test data: 0.7997483370665545\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "rforest = RandomForestRegressor(n_estimators=100, random_state=13)\n",
    "params = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "rforest = test_model(XS, y, rforest, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "best params: {'C': 10.0}\n",
      "best score: -22.264751071454743\n",
      "MSE on training data: 19.0102406255322\n",
      "MSE on test data: 15.667772910419604\n",
      "R^2 on training data: 0.7763641422557034\n",
      "R^2 on test data: 0.8082521582989659\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "svr = SVR(gamma=\"scale\")\n",
    "params = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "svr = test_model(XS, y, svr, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 14 features.\n",
      "[AutoFeat] With 404 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 11 transformed features from 2 original features - done.\n",
      "[feateng] Generated altogether 11 new features in 1 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 4 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 5 features after 5 feature selection runs\n",
      "[featsel] 5 features after correlation filtering\n",
      "[featsel] 4 features after noise filtering\n",
      "[AutoFeat] Computing 2 new features.\n",
      "[AutoFeat]     2/    2 new features ...done.\n",
      "[AutoFeat] Final dataframe with 4 feature columns (2 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "19.198310923011356\n",
      "48.671633 * 1/x001\n",
      "-0.372896 * x001\n",
      "0.031808 * x000\n",
      "0.003263 * exp(x000)\n",
      "[AutoFeat] Final score: 0.7369\n",
      "[AutoFeat] Computing 2 new features.\n",
      "[AutoFeat]     2/    2 new features ...done.\n",
      "autofeat new features: 2\n",
      "autofeat MSE on training data: 22.366797604568557\n",
      "autofeat MSE on test data: 20.19378940510691\n",
      "autofeat R^2 on training data: 0.7368777141846024\n",
      "autofeat R^2 on test data: 0.7528611401037495\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -24.050489014858318\n",
      "MSE on training data: 22.334797242311623\n",
      "MSE on test data: 19.969445653595734\n",
      "R^2 on training data: 0.7372541654143594\n",
      "R^2 on test data: 0.7556067396473051\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.01}\n",
      "best score: -21.444576255728133\n",
      "MSE on training data: 12.847390898639864\n",
      "MSE on test data: 16.35898987235349\n",
      "R^2 on training data: 0.8488637077252588\n",
      "R^2 on test data: 0.7997927964384283\n",
      "# SVR\n",
      "best params: {'C': 10.0}\n",
      "best score: -23.249465043624838\n",
      "MSE on training data: 17.292913714061946\n",
      "MSE on test data: 16.18011190177037\n",
      "R^2 on training data: 0.7965667206680038\n",
      "R^2 on test data: 0.8019819693976791\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "test_autofeat(XS, y, _, feateng_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 105 features.\n",
      "[AutoFeat] With 404 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 11 transformed features from 2 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 70 feature combinations from 78 original feature tuples - done.\n",
      "[feateng] Generated altogether 82 new features in 2 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 22 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 9 features after 5 feature selection runs\n",
      "[featsel] 6 features after correlation filtering\n",
      "[featsel] 5 features after noise filtering\n",
      "[AutoFeat] Computing 3 new features.\n",
      "[AutoFeat]     3/    3 new features ...done.\n",
      "[AutoFeat] Final dataframe with 5 feature columns (3 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-3.2488320846989893\n",
      "42.709443 * 1/x001\n",
      "3.513847 * x000\n",
      "-0.022404 * x001\n",
      "0.002333 * exp(x000)\n",
      "-0.000029 * x001**2*exp(x000)\n",
      "[AutoFeat] Final score: 0.7518\n",
      "[AutoFeat] Computing 3 new features.\n",
      "[AutoFeat]     3/    3 new features ...done.\n",
      "autofeat new features: 3\n",
      "autofeat MSE on training data: 21.097859284377073\n",
      "autofeat MSE on test data: 20.44931732005856\n",
      "autofeat R^2 on training data: 0.7518054636671361\n",
      "autofeat R^2 on test data: 0.7497338975488257\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -24.00806228151955\n",
      "MSE on training data: 21.07449787897942\n",
      "MSE on test data: 20.553839237139808\n",
      "R^2 on training data: 0.7520802864869587\n",
      "R^2 on test data: 0.7484547207235441\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.01}\n",
      "best score: -21.633924290697784\n",
      "MSE on training data: 12.572875284971719\n",
      "MSE on test data: 16.373935975534962\n",
      "R^2 on training data: 0.85209310055285\n",
      "R^2 on test data: 0.7996098806505065\n",
      "# SVR\n",
      "best params: {'C': 25.0}\n",
      "best score: -22.790812259614103\n",
      "MSE on training data: 16.1871281438122\n",
      "MSE on test data: 15.56109495482577\n",
      "R^2 on training data: 0.8095751464609913\n",
      "R^2 on test data: 0.8095577214992464\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "test_autofeat(XS, y, _, feateng_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### boston\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 2289 features.\n",
      "[AutoFeat] With 404 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 11 transformed features from 2 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 304 feature combinations from 78 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 1354 transformed features from 304 original features - done.\n",
      "[feateng] Generated altogether 1743 new features in 3 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 811 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 17 features after 5 feature selection runs\n",
      "[featsel] 7 features after correlation filtering\n",
      "[featsel] 6 features after noise filtering\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "[AutoFeat] Final dataframe with 7 feature columns (5 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "19.033773501174625\n",
      "100396.331661 * exp(-x000)/x001**3\n",
      "-0.149226 * x001\n",
      "-0.037371 * 1/(sqrt(x000) - x001)\n",
      "0.006080 * (x000**2 - x001)**2\n",
      "-0.001269 * exp(sqrt(x000)*log(x001))\n",
      "[AutoFeat] Final score: 0.7660\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "autofeat new features: 5\n",
      "autofeat MSE on training data: 19.89001340773024\n",
      "autofeat MSE on test data: 17.66195724190965\n",
      "autofeat R^2 on training data: 0.7660145236137018\n",
      "autofeat R^2 on test data: 0.7838466129988461\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 250.0}\n",
      "best score: -22.62266423170693\n",
      "MSE on training data: 20.147789183930623\n",
      "MSE on test data: 17.993523478208225\n",
      "R^2 on training data: 0.7629820576943147\n",
      "R^2 on test data: 0.7797887861108328\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.01}\n",
      "best score: -22.772186048819286\n",
      "MSE on training data: 12.361367535807375\n",
      "MSE on test data: 17.096751625396926\n",
      "R^2 on training data: 0.8545812708940714\n",
      "R^2 on test data: 0.7907638026787873\n",
      "# SVR\n",
      "best params: {'C': 25.0}\n",
      "best score: -23.385247329059787\n",
      "MSE on training data: 15.85256403745603\n",
      "MSE on test data: 15.556884017059108\n",
      "R^2 on training data: 0.8135109478203346\n",
      "R^2 on test data: 0.8096092564706117\n"
     ]
    }
   ],
   "source": [
    "print(\"####\", dsname)\n",
    "test_autofeat(XS, y, _, feateng_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
